{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing Claude with the API Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a Request Lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load env Variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an API Client\n",
    "from anthropic import Anthropic\n",
    "client = Anthropic()\n",
    "model = \"claude-sonnet-4-0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make a request\n",
    "message = client.messages.create(\n",
    "    model=model,\n",
    "    max_tokens=1000,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is quantum computing? Answer in one sentence\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='msg_01UFCkiFWRQRH1TPFxUZy3k3', content=[TextBlock(citations=None, text='Quantum computing is a revolutionary computing paradigm that uses quantum mechanical phenomena like superposition and entanglement to process information in ways that can potentially solve certain complex problems exponentially faster than classical computers.', type='text')], model='claude-sonnet-4-20250514', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=16, output_tokens=43, server_tool_use=None, service_tier='standard'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quantum computing is a revolutionary computing paradigm that uses quantum mechanical phenomena like superposition and entanglement to process information in ways that can potentially solve certain complex problems exponentially faster than classical computers.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message.content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Turn Conversations Lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load env Variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an API Client\n",
    "from anthropic import Anthropic\n",
    "client = Anthropic()\n",
    "model = \"claude-sonnet-4-0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_user_message(messages, text):\n",
    "    \"\"\"\n",
    "    Add a user message to the messages list.\n",
    "    \"\"\"\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": text\n",
    "    }\n",
    "    messages.append(user_message)\n",
    "\n",
    "def add_assistant_message(messages, text):\n",
    "    \"\"\"\n",
    "    Add an assistant message to the messages list.\n",
    "    \"\"\"\n",
    "    assistant_message = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": text\n",
    "    }\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "\n",
    "def chat(messages):\n",
    "    \"\"\"\n",
    "    Send a chat message to the model and return the response.\n",
    "    \"\"\"\n",
    "    message = client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=1000,\n",
    "        messages=messages\n",
    "    )\n",
    "    return message.content[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Define quantum computing in one sentence'}]\n",
      "[{'role': 'user', 'content': 'Define quantum computing in one sentence'}, {'role': 'assistant', 'content': 'Quantum computing is a revolutionary computing paradigm that harnesses quantum mechanical phenomena like superposition and entanglement to process information in ways that can potentially solve certain complex problems exponentially faster than classical computers.'}, {'role': 'user', 'content': 'Write another sentence'}]\n",
      "Quantum computers use quantum bits (qubits) that can exist in multiple states simultaneously, unlike classical bits that are either 0 or 1, enabling them to perform many calculations in parallel.\n"
     ]
    }
   ],
   "source": [
    "# Start with an empty message list\n",
    "messages = []\n",
    "\n",
    "# Add the initial user question\n",
    "add_user_message(messages, \"Define quantum computing in one sentence\")\n",
    "\n",
    "# Get Claude's response\n",
    "answer = chat(messages)\n",
    "print(messages)\n",
    "\n",
    "# Add Claude's response to the conversation history\n",
    "add_assistant_message(messages, answer)\n",
    "\n",
    "# Add a follow-up question\n",
    "add_user_message(messages, \"Write another sentence\")\n",
    "print(messages)\n",
    "\n",
    "# Get the follow-up response with full context\n",
    "final_answer = chat(messages)\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a chat that can have a conversation to do some simple maths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env Variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Create an API Client\n",
    "from anthropic import Anthropic\n",
    "client = Anthropic()\n",
    "model = \"claude-sonnet-4-0\"\n",
    "\n",
    "\n",
    "# Functions for handling messaging \n",
    "def add_user_message(messages, text):\n",
    "    \"\"\"\n",
    "    Add a user message to the messages list.\n",
    "    \"\"\"\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": text\n",
    "    }\n",
    "    messages.append(user_message)\n",
    "\n",
    "def add_assistant_message(messages, text):\n",
    "    \"\"\"\n",
    "    Add an assistant message to the messages list.\n",
    "    \"\"\"\n",
    "    assistant_message = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": text\n",
    "    }\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "\n",
    "def chat(messages):\n",
    "    \"\"\"\n",
    "    Send a chat message to the model and return the response.\n",
    "    \"\"\"\n",
    "    message = client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=1000,\n",
    "        messages=messages\n",
    "    )\n",
    "    return message.content[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> What is 5+10\n",
      "5 + 10 = 15\n",
      "Continuing the conversation...\n",
      "> minus 5 from that answer\n",
      "15 - 5 = 10\n",
      "Continuing the conversation...\n",
      "> add 5 to the answer\n",
      "10 + 5 = 15\n",
      "Continuing the conversation...\n",
      "> exit\n",
      "Goodbye! Feel free to come back if you need help with more math problems or anything else.\n",
      "Exiting the chat.\n"
     ]
    }
   ],
   "source": [
    "# Create an initial list of messages\n",
    "messages = []\n",
    "# Use a while true loop to keep the conversation going\n",
    "\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"> \")\n",
    "    print(\">\", user_input)\n",
    "    # Add user message to the conversation\n",
    "    add_user_message(messages, user_input)\n",
    "    # Get the model's response\n",
    "    response = chat(messages)\n",
    "    # Print the response    \n",
    "    print(response)\n",
    "    # Add the model's response to the conversation\n",
    "    add_assistant_message(messages, response)\n",
    "    # Check if the user wants to exit\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"stop\"]:\n",
    "        print(\"Exiting the chat.\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"Continuing the conversation...\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompt Lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env Variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Create an API Client\n",
    "from anthropic import Anthropic\n",
    "client = Anthropic()\n",
    "model = \"claude-sonnet-4-0\"\n",
    "\n",
    "\n",
    "# Functions for handling messaging \n",
    "def add_user_message(messages, text):\n",
    "    \"\"\"\n",
    "    Add a user message to the messages list.\n",
    "    \"\"\"\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": text\n",
    "    }\n",
    "    messages.append(user_message)\n",
    "\n",
    "def add_assistant_message(messages, text):\n",
    "    \"\"\"\n",
    "    Add an assistant message to the messages list.\n",
    "    \"\"\"\n",
    "    assistant_message = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": text\n",
    "    }\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "\n",
    "def chat(messages, system=None):\n",
    "    \"\"\"\n",
    "    Send a chat message to the model and return the response.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": messages\n",
    "    }\n",
    "    if system:\n",
    "        params[\"system\"] = system\n",
    "\n",
    "    message = client.messages.create(\n",
    "        **params\n",
    "    )\n",
    "    return message.content[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy to help you solve this equation step by step! Let's work through it together.\n",
      "\n",
      "First, let me ask you: what do you think our goal is when solving an equation like 5x + 2 = 12?\n",
      "\n",
      "Take a moment to think about what we're trying to find, and what it means for both sides of the equation to be equal.\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "system_prompt = \"\"\"\n",
    "You are a patient math tutor.\n",
    "Do not directly answer a student's questions.\n",
    "Guide them to a solution step by step.\n",
    "\"\"\"\n",
    "\n",
    "add_user_message(messages, \"Solve the equation 5x+2=12\")\n",
    "response = chat(messages, system=system_prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Prompt Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env Variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Create an API Client\n",
    "from anthropic import Anthropic\n",
    "client = Anthropic()\n",
    "model = \"claude-sonnet-4-0\"\n",
    "\n",
    "\n",
    "# Functions for handling messaging \n",
    "def add_user_message(messages, text):\n",
    "    \"\"\"\n",
    "    Add a user message to the messages list.\n",
    "    \"\"\"\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": text\n",
    "    }\n",
    "    messages.append(user_message)\n",
    "\n",
    "def add_assistant_message(messages, text):\n",
    "    \"\"\"\n",
    "    Add an assistant message to the messages list.\n",
    "    \"\"\"\n",
    "    assistant_message = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": text\n",
    "    }\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "\n",
    "def chat(messages, system=None):\n",
    "    \"\"\"\n",
    "    Send a chat message to the model and return the response.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": messages\n",
    "    }\n",
    "    if system:\n",
    "        params[\"system\"] = system\n",
    "\n",
    "    message = client.messages.create(\n",
    "        **params\n",
    "    )\n",
    "    return message.content[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an initial list of messages\n",
    "messages = []\n",
    "# Use a while true loop to keep the conversation going\n",
    "system_prompt = \"\"\"\n",
    "You are a patient math tutor.\n",
    "Do not directly answer a student's questions.\n",
    "Guide them to a solution step by step. Be as concise as possible.\n",
    "\"\"\"\n",
    "\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"> \")\n",
    "    print(\">\", user_input)\n",
    "    # Add user message to the conversation\n",
    "    add_user_message(messages, user_input)\n",
    "    # Get the model's response\n",
    "    response = chat(messages, system_prompt)\n",
    "    # Print the response    \n",
    "    print(response)\n",
    "    # Add the model's response to the conversation\n",
    "    add_assistant_message(messages, response)\n",
    "    # Check if the user wants to exit\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"stop\"]:\n",
    "        print(\"Exiting the chat.\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"Continuing the conversation...\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature Lesson \n",
    "Value must be between 0 and 1. 0 is more deterministic and probable, and conversely 1 is more creative. Make sure you research what is best for your application, i.e., 0 is better for coding, and factually correct info, 1 is better for creative writing, brainstorming, marketing content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env Variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Create an API Client\n",
    "from anthropic import Anthropic\n",
    "client = Anthropic()\n",
    "model = \"claude-sonnet-4-0\"\n",
    "\n",
    "\n",
    "# Functions for handling messaging \n",
    "def add_user_message(messages, text):\n",
    "    \"\"\"\n",
    "    Add a user message to the messages list.\n",
    "    \"\"\"\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": text\n",
    "    }\n",
    "    messages.append(user_message)\n",
    "\n",
    "def add_assistant_message(messages, text):\n",
    "    \"\"\"\n",
    "    Add an assistant message to the messages list.\n",
    "    \"\"\"\n",
    "    assistant_message = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": text\n",
    "    }\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "\n",
    "def chat(messages, system=None, temperature=0.0):\n",
    "    \"\"\"\n",
    "    Send a chat message to the model and return the response.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    if system:\n",
    "        params[\"system\"] = system\n",
    "\n",
    "    message = client.messages.create(\n",
    "        **params\n",
    "    )\n",
    "    return message.content[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A retired CIA operative discovers their own memories have been systematically erased and sold on the black market, forcing them to buy back pieces of their past to uncover why they've been hunting their own daughter.\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "\n",
    "add_user_message(messages, \"Give me a one line idea for a feature film\")\n",
    "response = chat(messages, temperature=1)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Streaming\n",
    "When building chat applications with Claude, there's a significant user experience challenge: responses can take 10-30 seconds to generate, leaving users staring at a loading spinner. The solution is response streaming, which lets users see text appear chunk by chunk as Claude generates it, creating a much more responsive feel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We wont use the chat function this time - \n",
    "# because we are going to stream the response as it comes backk\n",
    "\n",
    "# Load env Variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Create an API Client\n",
    "from anthropic import Anthropic\n",
    "client = Anthropic()\n",
    "model = \"claude-sonnet-4-0\"\n",
    "\n",
    "\n",
    "# Functions for handling messaging \n",
    "def add_user_message(messages, text):\n",
    "    \"\"\"\n",
    "    Add a user message to the messages list.\n",
    "    \"\"\"\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": text\n",
    "    }\n",
    "    messages.append(user_message)\n",
    "\n",
    "def add_assistant_message(messages, text):\n",
    "    \"\"\"\n",
    "    Add an assistant message to the messages list.\n",
    "    \"\"\"\n",
    "    assistant_message = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": text\n",
    "    }\n",
    "    messages.append(assistant_message)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RawMessageStartEvent(message=Message(id='msg_015UP7FZaoUWCbC3E4tYmzbb', content=[], model='claude-sonnet-4-20250514', role='assistant', stop_reason=None, stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=18, output_tokens=1, server_tool_use=None, service_tier='standard')), type='message_start')\n",
      "RawContentBlockStartEvent(content_block=TextBlock(citations=None, text='', type='text'), index=0, type='content_block_start')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='The', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' \"GlobalMind\" database is a fictional repository', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' containing the aggregated thoughts,', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' memories, and emotional', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' states of over 10 ', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='million individuals worldwide,', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' updated in real-time through neural', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' interface technology.', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockStopEvent(index=0, type='content_block_stop')\n",
      "RawMessageDeltaEvent(delta=Delta(stop_reason='end_turn', stop_sequence=None), type='message_delta', usage=MessageDeltaUsage(cache_creation_input_tokens=None, cache_read_input_tokens=None, input_tokens=None, output_tokens=44, server_tool_use=None))\n",
      "RawMessageStopEvent(type='message_stop')\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "add_user_message(messages, \"Write a 1 sentence description of a fake database\")\n",
    "\n",
    "stream = client.messages.create(\n",
    "    model=model,\n",
    "    max_tokens=1000,\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for event in stream:\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Global Unicorn Registry Database contains comprehensive records of all documented unicorn sightings, including their horn measurements, rainbow preferences, and magical ability classifications across 47 countries since 1892."
     ]
    }
   ],
   "source": [
    "# instead of parsing all the content manually, use\n",
    "# built in SDK streaming interface\n",
    "\n",
    "with client.messages.stream(\n",
    "    model=model,\n",
    "    max_tokens=1000,\n",
    "    messages=messages\n",
    ") as stream:\n",
    "    for text in stream.text_stream:\n",
    "        print(text, end=\"\") # The end=\"\" argument prevents new lines between chunks\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final msg: The \"GlobalMegaCorp Employee Database\" is a fictional relational database containing 50,000 synthetic employee records with fields for personal information, job titles, salaries, department assignments, and performance metrics across a imaginary multinational corporation's offices in 12 countries.\n"
     ]
    }
   ],
   "source": [
    "# Saving the complete response:\n",
    "with client.messages.stream(\n",
    "    model=model,\n",
    "    max_tokens=1000,\n",
    "    messages=messages\n",
    ") as stream:\n",
    "    for text in stream.text_stream:\n",
    "        # Send each chunk to your client\n",
    "        pass\n",
    "    \n",
    "    # Get the complete message for database storage\n",
    "    final_message = stream.get_final_message()\n",
    "\n",
    "print(f\"final msg: \"+final_message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlling Model Output\n",
    "Beyond crafting better prompts, there are two powerful techniques for controlling Claude's output: prefilled assistant messages and stop sequences. These methods give you precise control over how Claude responds and when it stops generating text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env Variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Create an API Client\n",
    "from anthropic import Anthropic\n",
    "client = Anthropic()\n",
    "model = \"claude-sonnet-4-0\"\n",
    "\n",
    "\n",
    "# Functions for handling messaging \n",
    "def add_user_message(messages, text):\n",
    "    \"\"\"\n",
    "    Add a user message to the messages list.\n",
    "    \"\"\"\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": text\n",
    "    }\n",
    "    messages.append(user_message)\n",
    "\n",
    "def add_assistant_message(messages, text):\n",
    "    \"\"\"\n",
    "    Add an assistant message to the messages list.\n",
    "    \"\"\"\n",
    "    assistant_message = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": text\n",
    "    }\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "\n",
    "def chat(messages, system=None, temperature=0.0):\n",
    "    \"\"\"\n",
    "    Send a chat message to the model and return the response.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    if system:\n",
    "        params[\"system\"] = system\n",
    "\n",
    "    message = client.messages.create(\n",
    "        **params\n",
    "    )\n",
    "    return message.content[0].text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefilled Assistant Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for breakfast if you want a stronger energy boost, as it contains more caffeine (95-200mg per cup vs. 25-50mg in tea). It can help you feel more alert and focused to start your day.\n",
      "\n",
      "Tea might be better if you prefer:\n",
      "- A gentler, more sustained energy lift\n",
      "- Less risk of jitters or crashes\n",
      "- Antioxidants and potentially calming compounds like L-theanine\n",
      "- Something easier on your stomach\n",
      "\n",
      "Both can be part of a healthy breakfast routine. The \"better\" choice really depends on:\n",
      "- Your caffeine tolerance\n",
      "- How your body reacts to each\n",
      "- Your taste preferences\n",
      "- Whether you're pairing it with food\n",
      "\n",
      "If you're unsure, you might try alternating between them or having tea on days when you want something milder. What matters most is what makes you feel good and fits your morning routine!\n"
     ]
    }
   ],
   "source": [
    "# We can essentially inject a message as part of the Agents output to control\n",
    "# or influence its response to the user. \n",
    "\n",
    "\n",
    "messages = []\n",
    "\n",
    "add_user_message(messages, \"Is tea or coffee better for breakfast?\")\n",
    "\n",
    "add_assistant_message(messages,\"Coffee is better\")\n",
    "\n",
    "# Now we can ask the agent to write a response that includes a message\n",
    "response = chat(messages)\n",
    "\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env Variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Create an API Client\n",
    "from anthropic import Anthropic\n",
    "client = Anthropic()\n",
    "model = \"claude-sonnet-4-0\"\n",
    "\n",
    "\n",
    "# Functions for handling messaging \n",
    "def add_user_message(messages, text):\n",
    "    \"\"\"\n",
    "    Add a user message to the messages list.\n",
    "    \"\"\"\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": text\n",
    "    }\n",
    "    messages.append(user_message)\n",
    "\n",
    "def add_assistant_message(messages, text):\n",
    "    \"\"\"\n",
    "    Add an assistant message to the messages list.\n",
    "    \"\"\"\n",
    "    assistant_message = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": text\n",
    "    }\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "\n",
    "def chat(messages, system=None, temperature=0.0, stop_sequences=[]):\n",
    "    \"\"\"\n",
    "    Send a chat message to the model and return the response.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature, \n",
    "        \"stop_sequences\": stop_sequences\n",
    "    }\n",
    "    if system:\n",
    "        params[\"system\"] = system\n",
    "\n",
    "    message = client.messages.create(\n",
    "        **params\n",
    "    )\n",
    "    return message.content[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the numbers 1 to 10 in order:\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can use stop sequences to hault/stop the model's response if it starts, \n",
    "# responding with content that is within that stop sequence\n",
    "\n",
    "\n",
    "messages = []\n",
    "stop_sequences = [\"7\"]\n",
    "add_user_message(messages, \"display numbers 1 to 10 in order\")\n",
    "\n",
    "# Now we can ask the agent to write a response that includes a message\n",
    "response = chat(messages, stop_sequences=stop_sequences)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Data\n",
    "When you need Claude to generate structured data like JSON, Python code, or bulleted lists, you'll often run into a common problem: Claude wants to be helpful and add explanatory text around your content. While this is usually great, sometimes you need just the raw data with nothing else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "  \"Name\": \"OrderProcessingRule\",\n",
      "  \"EventPattern\": {\n",
      "    \"source\": [\"myapp.orders\"],\n",
      "    \"detail-type\": [\"Order Placed\"]\n",
      "  },\n",
      "  \"Targets\": [\n",
      "    {\n",
      "      \"Id\": \"1\",\n",
      "      \"Arn\": \"arn:aws:lambda:us-east-1:123456789012:function:ProcessOrder\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" We are going to use both pre filled response, and stop sequences to help us handle the\n",
    "response from the model.\n",
    "\"\"\"\n",
    "\n",
    "messages = []\n",
    "\n",
    "add_user_message(messages, \"Generate a very short event bridge rule as json\")\n",
    "add_assistant_message(messages, \"\"\"```json\"\"\")\n",
    "stop_sequences = [\"```\"]\n",
    "# Now we can ask the agent to write a response that includes a message\n",
    "response = chat(messages, stop_sequences=stop_sequences)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "{'Name': 'OrderProcessingRule', 'EventPattern': {'source': ['myapp.orders'], 'detail-type': ['Order Placed']}, 'Targets': [{'Id': '1', 'Arn': 'arn:aws:lambda:us-east-1:123456789012:function:ProcessOrder'}]}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "print(type(response))\n",
    "json_response = json.loads(response) \n",
    "print(json_response)\n",
    "print(type(json_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Data Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are three short AWS CLI commands:\\n\\n1. **List S3 buckets:**\\n   ```bash\\n   aws s3 ls\\n   ```\\n\\n2. **Describe EC2 instances:**\\n   ```bash\\n   aws ec2 describe-instances\\n   ```\\n\\n3. **Get caller identity:**\\n   ```bash\\n   aws sts get-caller-identity\\n   ```'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = []\n",
    "\n",
    "prompt = \"\"\"\n",
    "    Generate three different sample AWS CLI commands. Each should be very short\n",
    "    \"\"\"\n",
    "\n",
    "add_user_message(messages, prompt)\n",
    "text = chat(messages)\n",
    "text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are three short AWS CLI commands:\n",
       "\n",
       "1. **List S3 buckets:**\n",
       "   ```bash\n",
       "   aws s3 ls\n",
       "   ```\n",
       "\n",
       "2. **Describe EC2 instances:**\n",
       "   ```bash\n",
       "   aws ec2 describe-instances\n",
       "   ```\n",
       "\n",
       "3. **Get caller identity:**\n",
       "   ```bash\n",
       "   aws sts get-caller-identity\n",
       "   ```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise!\n",
    "- Use messages prefilling and stop sequences to *only* get three different commands in a single response. \n",
    "- There shouldnt be any comments or explanation\n",
    "- Hint: message prefilling isnt limit to characters like ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aws s3 ls\\naws ec2 describe-instances\\naws iam list-users'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = []\n",
    "\n",
    "prompt = \"\"\"\n",
    "    Generate three different sample AWS CLI commands. Each should be very short\n",
    "    \"\"\"\n",
    "\n",
    "add_user_message(messages, prompt)\n",
    "add_assistant_message(messages, \"\"\"Here are all three commands in a single block without any comments ```bash\"\"\")\n",
    "stop_sequences = [\"```\"]\n",
    "text = chat(messages, stop_sequences=stop_sequences)\n",
    "text.strip()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
